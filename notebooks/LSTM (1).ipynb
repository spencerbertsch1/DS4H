{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["## Hey There!\n","Modeling Approach: Long Short Term Memory Neural Network <br>\n","@Spencer your code is in the 1D_CNN File"],"metadata":{"id":"MT5-4NsQnuRS"}},{"cell_type":"markdown","source":["# Set up"],"metadata":{"id":"IuDjpR7WoAAX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fb7-vaTFl-Y5"},"outputs":[],"source":["# @title Imports \n","import pandas as pd\n","import numpy as np\n","\n","import tensorflow as tf\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import class_weight\n","\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Dropout\n","from keras.layers.embeddings import Embedding\n","from keras.metrics import AUC\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","import random\n","import os\n","import json\n","\n","\n","\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output \n","\n","pd.set_option('display.max_columns', None)"]},{"cell_type":"markdown","source":["Connect to TPU"],"metadata":{"id":"rlxw5ORUoHWe"}},{"cell_type":"code","source":["## Connect to TPU ##\n","\n","# TPU Configuration\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n","tf.config.experimental_connect_to_cluster(resolver)\n","# This is the TPU initialization code that has to be at the beginning.\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n","\n","a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n","b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n","\n","with tf.device('/TPU:0'):\n","  c = tf.matmul(a, b)\n","print(\"c device: \", c.device)\n","print(c)\n","strategy = tf.distribute.TPUStrategy(resolver)\n","\n","# Clear ouput \n","clear_output()"],"metadata":{"id":"nBxPRS9MoKW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Connect to Drive \n","from google.colab import drive\n","drive.mount('/content/drive')\n","clear_output()"],"metadata":{"id":"_oPw0q2Io2sr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Data Preprocessing / Normalization "],"metadata":{"id":"aDJkAzLjpQ9U"}},{"cell_type":"markdown","source":["Get and Preprocess X"],"metadata":{"id":"Pkj3ayicphBu"}},{"cell_type":"markdown","source":["We should come up with a better long-term solution for the issue of not being able to load data easily :|\n"],"metadata":{"id":"Kvu595a-I_Yh"}},{"cell_type":"code","source":["file_location_spencer: str = '/content/drive/MyDrive/DS4H/Data/wideX_Smoothed.csv'\n","file_location_franklin: str = '/content/drive/MyDrive/Colab Notebooks/Sleep Disorder Project/Project Code + Data/Minute-by-Minute Data/Wide Data [Smoothed]/wideX_Smoothed.csv'"],"metadata":{"id":"3-qD6YrAI4Si"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load data\n","Wide_X = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Sleep Disorder Project/Project Code + Data/Minute-by-Minute Data/Wide Data [Smoothed]/wideX_Smoothed.csv')\n","Wide_X = Wide_X.drop(\"Unnamed: 0\", axis = 1)\n","\n","#Wide_X.fillna(0)\n","Wide_X = Wide_X.where(Wide_X > 0.0000001, 0)"],"metadata":{"id":"Z0NuF3TupZAh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Wide_X"],"metadata":{"id":"5fx9pBAGTFs_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Seperate SEQN from dataframe\n","SEQN = Wide_X.pop(\"SEQN\")\n","data_wide = Wide_X\n","\n","# Normalize from 0.01 to 1\n","scaler = MinMaxScaler()\n","scaler.fit(data_wide)\n","data_wide = scaler.transform(data_wide)\n","\n","# Convert DF to array\n","data_wide = np.array(data_wide)\n","\n","# Set 3-D shape\n","data_wide = data_wide.reshape(len(data_wide),len(data_wide[0]),1)\n","data_wide = data_wide.astype(np.float32)\n","# Set shape for model\n","data_wide_shape = (data_wide.shape[1],data_wide.shape[2])\n","\n","\n","\n","print(\"Shape:\", data_wide.shape)"],"metadata":{"id":"csilG7u_p90w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preprocess y"],"metadata":{"id":"mp4LVKNLrdC3"}},{"cell_type":"code","source":["# Load Data Y ----\n","Y = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Sleep Disorder Project/Project Code + Data/Minute-by-Minute Data/Wide Data [Smoothed]/y.csv')\n","Y = Y.drop(\"Unnamed: 0\", axis = 1)\n","#Make y array as well \n","y = np.hstack(np.asarray(Y)).reshape(len(Y),1)\n","print(y.shape)\n","y = y.astype(np.float32)"],"metadata":{"id":"fEcl-KVYreHJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Stratified Train Test Split"],"metadata":{"id":"s4XyQR0YrdzF"}},{"cell_type":"code","source":["# Train Test Split\n","X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(data_wide, y, SEQN, test_size=0.2, stratify = y, random_state = 41)\n","\n","# Train Validation Split\n","X_train, X_val, y_train, y_val, ids_train, ids_val = train_test_split(X_train, y_train, ids_train, test_size=0.2, stratify = y_train, random_state = 32)"],"metadata":{"id":"OW6mDMeGrfhG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modeling!"],"metadata":{"id":"Ph24-PQvsNSG"}},{"cell_type":"markdown","source":["Make Model"],"metadata":{"id":"zO2pxbBhskd-"}},{"cell_type":"code","source":["#Make Model \n","def create_model():\n","  model = tf.keras.Sequential()\n","  model.add(tf.keras.layers.LSTM(\n","            units=400, # adjust size \n","          input_shape=(10080, 1)\n","          \n","      )\n","  )\n","  model.add(Dense(100, activation='relu'))\n","  model.add(tf.keras.layers.Dropout(rate=0.2))\n","  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","  return model"],"metadata":{"id":"rCnEPAH1sQHF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compile Model "],"metadata":{"id":"wCIq_Kp-slVY"}},{"cell_type":"code","source":["from keras import optimizers\n","\n","# Compile Model \n","with strategy.scope():\n","  model = create_model()\n","  model.compile(\n","  loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),\n","  optimizer= tf.keras.optimizers.Adam(),\n","  metrics= tf.keras.metrics.AUC(name=\"auc\")\n",")"],"metadata":{"id":"6IXVi7U5su4N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save Model \n","model.save_weights(\"reset_weights.h5\")\n","\n","def reset_model():\n","  model.load_weights(\"reset_weights.h5\")"],"metadata":{"id":"r6qASxBvsXRB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Visuals"],"metadata":{"id":"_YXqqe8xsuYl"}},{"cell_type":"code","source":["# Model Summary\n","model.summary()"],"metadata":{"id":"0ddzksOiswDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot Model Architecture \n","from keras.utils.vis_utils import plot_model\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"],"metadata":{"id":"3ed9ZeNos2ZO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Train Model "],"metadata":{"id":"0l-19Xakszbp"}},{"cell_type":"code","source":["## Parameters##\n","\n","# Save Best Weights \n","filepath = \"VAL-AUC-impr(fr).h5\"\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath, #=checkpoint_filepath,\n","    verbose = 1,\n","    monitor='val_auc',\n","    mode='max',\n","    save_best_only=True)\n","\n","\n","# Balance Weights \n","n_sd = sum(y)\n","n_no_sd = len(y) - n_sd\n","class_weights = {0: (n_no_sd/len(y)),                # Custom Setting Class Weights \n","                1: (n_sd/len(y))}"],"metadata":{"id":"QbfcrQm4tBYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit model                                              \n","tf.keras.backend.clear_session()\n","history = model.fit(\n","    X_train, y_train,\n","    validation_data=(X_val, y_val),\n","    epochs=500, \n","    batch_size= 32, # Change size \n","    shuffle=False,\n","    verbose = 1,\n","    class_weight=class_weights,\n","    callbacks=[model_checkpoint_callback]\n",")"],"metadata":{"id":"pyAyHylDsjON"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Graphing"],"metadata":{"id":"gCqq5IvXommB"}},{"cell_type":"markdown","source":["Evaulate "],"metadata":{"id":"54MGnTYVtmPL"}},{"cell_type":"code","source":["history_df = pd.DataFrame({'auc':history.history['auc'], 'val_auc':history.history['val_auc']})\n","history_df.to_csv(\"history_df.csv\", index = False)\n","\n","# Plot history: AUC\n","plt.plot(history_df['auc'], label='Training')\n","plt.plot(history_df['val_auc'], label='Validation')\n","plt.title('AUC')\n","plt.ylabel('AUC')\n","plt.xlabel('No. epoch')\n","plt.legend(loc=\"upper left\")\n","plt.show()\n","plt.savefig('')"],"metadata":{"id":"vmgn6Myoooy4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate \n","model.evaluate(X_test, y_test, batch_size = 32)\n","#Keras Test AUC\n","from keras.models import load_model\n","test_set_model = keras.models.load_model('VAL-AUC-impr(fr).h5')\n","results = test_set_model.evaluate(X_test, y_test, batch_size = 32)\n","print(\"--------------------------\")\n","print(f'Keras Model Loading Test AUC: {round(results[1],2)}')\n","print(\"--------------------------\")\n","\n","\n","#SKLearn Test AUC \n","preds = test_set_model.predict(X_test)\n","from sklearn.metrics import r2_score, roc_auc_score\n","print(f'SKLearn Test AUC: {round(roc_auc_score(y_test, preds),3)}')\n","print(\"--------------------------\")\n","plt.plot(preds)\n","plt.title('Predictions for Test Set')\n","plt.show()\n","plt.scatter(y_test, preds)"],"metadata":{"id":"5a2o2ZaGtgdN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#K-Fold Cross Validation"],"metadata":{"id":"wQKh_5u8wygt"}},{"cell_type":"code","source":["%%time\n","from sklearn.model_selection import StratifiedKFold\n","\n","# Global Score List Buckets\n","cv_test_scores=[]\n","cv_val_scores=[]\n","\n","# K fold parameters \n","seed = 7\n","kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n","\n","# run K-fold \n","count = 1\n","for train, val in kfold.split(X_train, y_train):\n","  \n","  # Create New Training Set \n","  X_training = X_train[train]\n","  y_training = y_train[train]\n","  # Create new Validation Sets \n","  X_val = X_train[val]\n","  y_val = y_train[val]\n","\n","  # Reset model \n","  print(\"Resetting model..\")\n","  reset_model()\n","  # Checkpoints\n","  filepath = \"VAL-AUC-impr(fr)\" + str(count) +\".h5\"\n","\n","  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath, #=checkpoint_filepath,\n","    verbose = 2,\n","    monitor='val_auc',\n","    mode='max',\n","    save_best_only=True)\n","\n","  # fit model   \n","  tf.keras.backend.clear_session()\n","  history = model.fit(\n","      X_training, y_training,\n","      validation_data=(X_val, y_val),\n","      epochs=30, # Make Epochs larger\n","      batch_size= 32, # Change size \n","      shuffle=False,\n","      verbose = 1,\n","      class_weight=class_weights,\n","      callbacks=[model_checkpoint_callback]\n","  )\n","\n","\n"," \n","  # Eval model \n","  # model_eval()\n","  from keras.models import load_model\n","  test_set_model = keras.models.load_model(filepath)\n","  print(\" \\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\ntest set:\")\n","  scores = test_set_model.evaluate(X_test, y_test, batch_size=64) # Test Set\n","  cv_test_scores.append(scores[1])\n","  print(\" \\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nval set:\")\n","  scores = test_set_model.evaluate(X_val, y_val, batch_size=64)  # Validation Set\n","  cv_val_scores.append(scores[1])\n","\n","\n","\n","  # Save model \n","  print(\"Saving model...\")\n","  #save_model(count)\n","  # Save files\n","  print(\"Saving files...\")\n","  #save_files(count)\n","\n","  # increment \n","  print(\"FINISHED CYCLE NUMBER:\", count)\n","  count += 1 \n","\n","# Score Eval\n","print(\"\\nCV Test AUC----------------------------\")\n","print(\"Individual scores:\", cv_test_scores)\n","print(\"Mean:\", np.mean(cv_test_scores))\n","print(\"std:\", np.std(cv_test_scores))\n","print(\"\\nCV Val AUC-----------------------------\")\n","print(\"Individual scores:\", cv_val_scores)\n","print(\"Mean:\", np.mean(cv_val_scores))\n","print(\"std:\", np.std(cv_val_scores))"],"metadata":{"id":"UHJ6QWqQw-cZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Spencer Implementation"],"metadata":{"id":"xWlAwJNSJ2eE"}},{"cell_type":"code","source":["# lstm model\n","from numpy import mean\n","from numpy import std\n","from numpy import dstack\n","from pandas import read_csv\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from matplotlib import pyplot\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"],"metadata":{"id":"1u1iKk9oQ61Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load data\n","file_location_spencer: str = '/content/drive/MyDrive/DS4H/Data/wideX_Smoothed.csv'\n","Wide_X = pd.read_csv(file_location_spencer)\n","Wide_X = Wide_X.drop(\"Unnamed: 0\", axis = 1)\n","Wide_X = Wide_X.fillna(0)\n","Wide_X = round(Wide_X, 6)\n","Wide_X.head()"],"metadata":{"id":"XfFN4EVRTNrg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load response variable\n","file_location_spencer_y: str = '/content/drive/MyDrive/DS4H/Data/data_all_features_hourly.csv'\n","df_demographic = pd.read_csv(file_location_spencer_y)\n","df_demographic = df_demographic.drop(\"Unnamed: 0\", axis = 1)\n","df_demographic = df_demographic.fillna(0)\n","df_demographic = round(df_demographic, 6)\n","df_demographic.head()"],"metadata":{"id":"aGE9nmwjDt7f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Choose the response variable from the dataframe above - can be anything numeric"],"metadata":{"id":"PuAzp3ylT9kw"}},{"cell_type":"code","source":["response = 'sleep_disorder'"],"metadata":{"id":"DGu20J9dT9Z0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_response = df_demographic[['patient_id', response]]\n","df_response"],"metadata":{"id":"vm4TSjsDT9N0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def transform_y(y_df: pd.DataFrame) -> np.array:\n","  \"\"\"\n","  Used for cleaning\n","  \"\"\"\n","  y = np.hstack(np.asarray(y_df)).reshape(len(y_df), 1)\n","  print(y.shape)\n","  y = y.astype(np.int32)\n","\n","  return y"],"metadata":{"id":"QvG7lEPBdeOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def scale_data(data_wide: pd.DataFrame) -> np.array:\n","  \"\"\"\n","  Used for cleaning\n","  \"\"\"\n","  # Normalize from 0.01 to 1\n","  scaler = MinMaxScaler()\n","  scaler.fit(data_wide)\n","  data_wide = scaler.transform(data_wide)\n","\n","  # Convert DF to array\n","  data_wide = np.array(data_wide)\n","\n","  # Set 3-D shape\n","  data_wide = data_wide.reshape(len(data_wide),len(data_wide[0]),1)\n","  data_wide = data_wide.astype(np.float32)\n","  # Set shape for model\n","  data_wide_shape = (data_wide.shape[1],data_wide.shape[2])\n","\n","  print(\"Shape:\", data_wide.shape)\n","\n","  return data_wide"],"metadata":{"id":"pWGBoT4JbY3d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_data_set(df_minutes: pd.DataFrame, df_demographic: pd.DataFrame):\n","  \"\"\"\n","  This funciton takes the minute-level activity data and joins it onto a response variable, then creates the X_train, y_train, X_test, and y_test arrays\n","\n","  \"\"\"\n","\n","  # step 1: join the dataframes to get the response variable \n","  df_joined = df_minutes.merge(df_demographic, left_on='SEQN', right_on='patient_id', how='inner')\n","  df_joined = df_joined.drop_duplicates()\n","\n","  # remove the ID numbers \n","  if \"SEQN\" in df_joined.columns: \n","    SEQN = df_joined.pop(\"SEQN\")\n","  if \"patient_id\" in df_joined.columns: \n","    patient_id = df_joined.pop(\"patient_id\")\n","\n","  # Seperate y from dataframe\n","  y = df_joined.pop(response)\n","\n","  # define X\n","  X = df_joined.copy()\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)\n","\n","  # scale the feature set \n","  X_train = scale_data(data_wide=X_train)\n","  X_test = scale_data(data_wide=X_test)\n","\n","  # transform the response variable and reshape\n","  y_train = transform_y(y_train)\n","  y_test = transform_y(y_test)\n","\n","  return {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}  \n","\n","data_dict: dict = generate_data_set(df_minutes=Wide_X, df_demographic=df_response)\n","\n","X_train: np.array = data_dict['X_train']\n","X_test: np.array = data_dict['X_test']\n","y_train: np.array = data_dict['y_train']\n","y_test: np.array = data_dict['y_test']\n","\n","print(f'X_train shape: {X_train.shape}')\n","print(f'X_test shape: {X_test.shape}')\n","print(f'y_train shape: {y_train.shape}')\n","print(f'y_test shape: {y_test.shape}')"],"metadata":{"id":"ndnls0XpSCFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_cm(y_test, y_pred):\n","  \"\"\"\n","  generate confusion matrix\n","  \"\"\"\n","  cm = confusion_matrix(y_test, y_pred)\n","  disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","  disp.plot()\n","  plt.show()"],"metadata":{"id":"oZZvpvtfi16y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fit and evaluate a model\n","def evaluate_model(trainX, trainy, testX, testy):\n","  verbose, epochs, batch_size = 1, 5, 64\n","  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n","  model = Sequential()\n","  model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(100, activation='relu'))\n","  model.add(Dense(n_outputs, activation='softmax'))\n","  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\t\n","  # fit network\n","  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","  \n","  y_pred = model.predict(testX)\n","\n","  # evaluate model\n","  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n","    \n","  generate_cm(y_test, y_pred)\n","\n","  return accuracy\n"," \n","# summarize scores\n","def summarize_results(scores):\n","\tprint(scores)\n","\tm, s = mean(scores), std(scores)\n","\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n"],"metadata":{"id":"8DJlKBQkjthV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"vyU7eJD3jtUs"}},{"cell_type":"code","source":["score = evaluate_model(X_train, y_train, X_test, y_test)\n","score = score * 100.0\n","print(score)"],"metadata":{"id":"lAVDkvGxDxTN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # run an experiment\n","# def run_experiment(repeats=10):\n","# \t# load data\n","\n","#   # TODO add stochasticity back in here vvv\n","# \ttrainX, trainy, testX, testy = X_train, y_train, X_test, y_test\n","# \t# repeat experiment\n","# \tscores = list()\n","# \tfor r in range(repeats):\n","# \t\tscore = evaluate_model(trainX, trainy, testX, testy)\n","# \t\tscore = score * 100.0\n","# \t\tprint('>#%d: %.3f' % (r+1, score))\n","# \t\tscores.append(score)\n","# \t# summarize results\n","# \tsummarize_results(scores)\n"," \n","# # run the experiment\n","# run_experiment()  # <-- uncomment for experiment run "],"metadata":{"id":"kI7pqzEfDxxD"},"execution_count":null,"outputs":[]}]}